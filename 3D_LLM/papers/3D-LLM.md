- 输入：3D 点云及其特征
- 输出：语言序列（与指令相关）
- 任务：（主要是3D场景理解规划）执行一系列与 3D 相关的任务，包括caption、dense caption、3D 问答、任务分解、3D 基础、3D 辅助对话、导航等。
## 1 研究目标

将 3D 世界 inject into 大型语言模型中

## 2 主要贡献

- 3D-LLM
	- 关于整个场景的长期记忆可以存储在整体 3D 表示中，而不是片段的部分视图观察中。
	- 3D 属性（例如可供性和空间关系）可以从 3D 表示中推理出来，远远超出基于语言或基于 2D 图像的法学硕士的范围。

## 3 问题解决

主要困难：**三维数据稀缺** & **三维特征与语言特征对齐**

- **提出三维语言数据生成框架**：作者生成大规模的与语言配对的3D数据。**利用ChatGPT并设计了三种有效的提示程序**，用于3D数据和语言之间的交流。这种方式能够获取大约一百万条3D语言数据，涵盖了多种任务，包括但不限于3D字幕、密集字幕、3D问答、3D任务分解、3D基础、辅助对话、导航等。
- **利用二维特征**：与大规模预训练（如CLIP）不同，作者利用一个**3D特征提取器**从**多视角图像的预训练2D特征**中构建特征，这种方法与现有的视觉-语言模型是一致的，可以无缝地与**2D VLMs**整合，为3D-LLMs的高效训练提供支持。
- **三维位置信息**：作者开发了一种3D定位机制，弥合语言和空间位置之间的差距。具体地，作者在提取的**3D特征**上附加**3D位置嵌入**，以更好地编码空间信息。此外，作者在3D-LLMs中附加了一系列位置标记，可以通过输出位置标记来训练定位，给定场景中特定对象的语言描述，帮助3D-LLMs更好地捕捉3D空间信息。

## 4 一些背景

- VLM
	- 大规模视觉-语言数据预训练
   	- 额外模块连接视觉模型和LLM
 
- 一些常见3D任务
	- ScanQA：让模型回答与3D世界相关的问题。
	- ScanRefer：让模型定位文本表达所指的区域。
	- 3D captioning ：测试模型生成描述3D场景的字幕的能力。
	- 作者提出的3DLLM：上述3D任务及模型通常是任务限定的，并且只能处理训练集分布范围内的情况，无法进行泛化。与它们不同，作者构建了一个能够同时处理不同任务的3D模型，并实现新的能力，如3D助理对话和任务分解。


