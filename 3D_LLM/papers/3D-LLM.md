# 3D-LLM: Injecting the 3D World into Large Language Models

- 输入: 3D场景点云及其特征
- 输出: 语言序列（与指令相关）
- 问题: **三维数据稀缺** & **三维特征与语言特征对齐**
- 贡献: 
	- 3D-LLM模型
	- 数据集
	- 数据集构建方法
- 任务：（主要是3D场景理解规划）执行一系列与 3D 相关的任务，包括caption、dense caption、3D 问答、任务分解、3D 基础、3D 辅助对话、导航等。
## 1. 一些背景

- VLM
	- 目前已经有很多大规模视觉-语言数据预训练, 可以利用
	- 额外模块连接视觉模型和LLM
 
- 一些常见3D任务数据集（数量和多样性方面都受到限制）
	- ScanQA：让模型回答与3D世界相关的问题。
	- ScanRefer：让模型定位文本表达所指的区域。
	- 3D captioning ：测试模型生成描述3D场景的字幕的能力。
	- 作者提出的3DLLM：上述3D任务及模型通常是任务限定的，并且只能处理训练集分布范围内的情况，无法进行泛化。与它们不同，作者构建了一个能够同时处理不同任务的3D模型，并实现新的能力，如3D助理对话和任务分解。

## 2. 贡献

- **3D-LLM**
	- 关于整个场景的长期记忆可以存储在整体 3D 表示中，而不是片段的部分视图观察中。
	- 3D 属性（例如可供性和空间关系）可以从 3D 表示中推理出来，远远超出基于语言或基于 2D 图像的LLM的范围。
- **三维语言数据生成框架**：作者生成大规模的与语言配对的3D数据。**利用ChatGPT并设计了三种有效的提示程序**，用于3D数据和语言之间的交流。这种方式能够获取大约一百万条3D语言数据，涵盖了多种任务，包括但不限于3D字幕、密集字幕、3D问答、3D任务分解、3D基础、辅助对话、导航等。
- **利用二维特征**：与大规模预训练（如CLIP）不同，作者利用一个**3D特征提取器**从**多视角图像的预训练2D特征**中构建特征，这种方法与现有的视觉-语言模型是一致的，可以无缝地与**2D VLMs**整合，为3D-LLMs的高效训练提供支持。
- **三维位置信息**：作者开发了一种3D定位机制，弥合语言和空间位置之间的差距。具体地，作者在提取的**3D特征**上附加**3D位置嵌入**，以更好地编码空间信息。此外，作者在3D-LLMs中附加了一系列位置标记，可以通过输出位置标记来训练定位，给定场景中特定对象的语言描述，帮助3D-LLMs更好地捕捉3D空间信息。

## 3. 构建3D-语言数据集

- 基于**三维检测框（框-演示-指令）**的prompts。
	- 输入3D场景中房间和物体的三维框，并且提供有关场景的语义和空间位置的信息。然后向GPT模型提供具体的指令，以生成多样化的数据。
- 基于**ChatCaptioner**的prompts。
	- 作者**引导ChatGPT提出有关图像的一系列问题，让BLIP-2回答这些问题**。作者首先从3D场景的不同视角中抽几个图像，并输入到ChatGPT和BLIP-2中得到标题描述文本。，再利用ChatGPT总结所有这些标题文本，这些文本中包含有关不同区域的信息，形成整个场景的全局3D描述。
- 基于**修改**的prompts。
	- 它可用于将一种类型的3D数据转换为另一种类型。

## 4. 训练3D-LLM模型

![[Pasted image 20241120151447.png]]
### 4.1 使用2D方式提取3D特征

利用成熟的2D特征提取器（例如CLIP）
- 在几个不同的视图中**渲染3D场景成2D图像**
- 对2D图像的每个**像素**，利用**2D特征提取器，例如CLIP**提取特征
	- 特征通常为一个高维（如1024维）的向量，包含计算机可以理解的颜色等信息
- 2D特征映射到3D
	- **直接重建法**：使用真实相机参数直接从渲染的RGBD图像中**重建点云**，**特征直接映射到重建的3D点**。这种方法适用于**带相机姿态和内参的渲染RGBD数据**。
	- **特征融合**：作者用**gradslam**将**2D特征融合到3D映射中**（？），与密集映射方法不同，这些特征与深度和颜色一起融合。这种方法适用于具有**嘈杂深度图像渲染或嘈杂相机姿态和内参的3D数据**。
	- **神经场**：使用**神经体素场**构建3D紧凑表示。具体而言，场中的每个体素都有一个除了密度和颜色的特征。然后使用MSE损失在射线中对齐3D特征，在像素中对齐2D特征。这种方法适用于具有RGB渲染但没有深度数据以及嘈杂相机姿态和内参的3D数据。

### 4.2 训练3D-LLMs

>从头训练3D-LLMs不现实，需要使用2D提取：通常情况下，训练使用了约50亿张图像之后，2D VLMs的训练才开始逐渐有效。它们使用冻结3和预训练的图像编码器（如CLIP）来提取2D图像的特征。由于3D特征提取器可以将3D特征映射到与2D图像相同的特征空间，因此将这些预训练的2D VLMs用作主干简便而合理。

预训练大模型：Flamingo和BLIP-2

作者设计的2D特征提取器有以下要点：

- **感知器架构**：感知器架构利用不对称的注意机制将输入逐步提炼成紧凑的特征，因此能够处理任意大小的非常大的输入并处理不同的模态。这种架构被用于像Flamingo这样的VLMs中。BLIP-2也利用了一个名为QFormer的类似结构。由冻结的图像编码器输出的2D图像特征被展平并送到感知器中以生成固定大小的输入。
- **3D特征提取器与2D特征空间对齐**：由于3D特征与3D特征提取器将其映射到与2D图像相同的特征空间中，因此可以将具有任意大小的点云特征输入到感知器中。因此，作者使用3D特征提取器在与冻结图像编码器特征空间相同的特征空间中提取3D特征。
- **使用预训练的2D VLMs作为主干**：考虑到感知器可以处理相同特征维度的任意大小的输入，作者将对齐的3D特征输入到预训练的2D VLMs中，用收集的3D-语言数据集训练3D-LLMs。

**3D定位**。如何将物体的位置信息引入到LLM中是另一个问题，一个简单的思路是，通过已与语言对齐的2D预训练特征提取器（如CLIP和EVA-CLIP）**重建3D特征**后，可以通过直接计算3D特征与语言特征之间的相似性来进行定位。但是作者更希望模型本身能够捕获3D空间信息。作者提出了一种3D定位机制，增强了3D-LLMs吸收空间信息的能力。该机制包括两个部分：

（1）用位置嵌入增强3D特征；位置嵌入部分是将位置嵌入添加到从2D多视图特征聚合的3D特征中。

（2）用位置标记增强LLM词汇表。位置标记部分则是将3D位置嵌入到词汇表中，使得3D空间位置与LLMs对齐。







