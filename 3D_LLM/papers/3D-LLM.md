# 3D-LLM: Injecting the 3D World into Large Language Models

- 输入: 3D场景点云（处理获得特征）以及问题语言序列
- 输出: 语言序列（与指令相关）
- 问题: **三维数据稀缺** & **三维特征与语言特征对齐**
- 贡献: 
	- 3D-LLM模型
	- 数据集
	- 数据集构建方法
- 任务：（主要是3D场景理解规划）执行一系列与 3D 相关的任务，包括caption、dense caption、3D 问答、任务分解、3D 基础、3D 辅助对话、导航等。
## 1. 一些背景

- VLM
	- 目前已经有很多大规模视觉-语言数据预训练, 可以利用
	- 额外模块连接视觉模型和LLM
 
- 一些常见3D任务数据集（数量和多样性方面都受到限制）
	- ScanQA：让模型回答与3D世界相关的问题。
	- ScanRefer：让模型定位文本表达所指的区域。
	- 3D captioning ：测试模型生成描述3D场景的字幕的能力。
	- 作者提出的3DLLM：上述3D任务及模型通常是任务限定的，并且只能处理训练集分布范围内的情况，无法进行泛化。与它们不同，作者构建了一个能够同时处理不同任务的3D模型，并实现新的能力，如3D助理对话和任务分解。

## 2. 贡献

- **3D-LLM**
	- 关于整个场景的长期记忆可以存储在整体 3D 表示中，而不是片段的部分视图观察中。
	- 3D 属性（例如可供性和空间关系）可以从 3D 表示中推理出来，远远超出基于语言或基于 2D 图像的LLM的范围。
- **三维语言数据生成框架**：作者生成大规模的与语言配对的3D数据。**利用ChatGPT并设计了三种有效的提示程序**，用于3D数据和语言之间的交流。这种方式能够获取大约一百万条3D语言数据，涵盖了多种任务，包括但不限于3D字幕、密集字幕、3D问答、3D任务分解、3D基础、辅助对话、导航等。
- **利用二维特征**：与大规模预训练（如CLIP）不同，作者利用一个**3D特征提取器**从**多视角图像的预训练2D特征**中构建特征，这种方法与现有的视觉-语言模型是一致的，可以无缝地与**2D VLMs**整合，为3D-LLMs的高效训练提供支持。
- **三维位置信息**：作者开发了一种3D定位机制，弥合语言和空间位置之间的差距。具体地，作者在提取的**3D特征**上附加**3D位置嵌入**，以更好地编码空间信息。此外，作者在3D-LLMs中附加了一系列位置标记，可以通过输出位置标记来训练定位，给定场景中特定对象的语言描述，帮助3D-LLMs更好地捕捉3D空间信息。

## 3. 构建3D-语言数据集

- 基于**三维检测框（框-演示-指令）的prompts**。
	- 输入3D场景中房间和物体的三维框，并且提供有关场景的语义和空间位置的信息。然后向GPT模型提供具体的指令，以生成多样化的数据。
- 基于**ChatCaptioner**的prompts。
	- 作者**引导ChatGPT提出有关图像的一系列问题，让BLIP-2回答这些问题**。作者首先从3D场景的不同视角中抽几个图像，并输入到ChatGPT和BLIP-2中得到标题描述文本。，再利用ChatGPT总结所有这些标题文本，这些文本中包含有关不同区域的信息，形成整个场景的全局3D描述。
	- **ChatCaptioner** 是一种结合了 **ChatGPT** 和 **BLIP-2** 模型的技术，用于生成3D场景的详细描述。具体来说，ChatCaptioner通过多轮对话的形式，利用ChatGPT提出关于3D场景的问题，BLIP-2回答这些问题，最终形成对整个3D场景的全局描述。这种方法能够生成高质量的3D-语言数据，用于训练3D-LLM模型。
- 基于**修改**的prompts。
	- 它可用于将一种类型的3D数据转换为另一种类型。


- 几个原始资产
	- Objaverse  一个由 800K 3D 对象组成的宇宙。
		- 然而，由于语言描述是从在线资源中提取的，并且未经人类检查，因此大多数对象都有非常嘈杂的描述（例如，带有 url）或没有描述。我们利用基于 ChatCaptioner 的提示来生成场景的高质量 3D 相关描述。
	- Scannet 
		- 是一个包含约 1k 3D 室内场景的注释丰富的数据集。它提供场景中对象的语义和边界框。
	- Habitat-Matterport (HM3D)
		- 是体现人工智能的 3D 环境数据集。 HM3DSem 进一步为HM3D的200多个场景添加了语义注释和边界框。

## 4. 训练3D-LLM模型

![[Pasted image 20241120151447.png]]
前两列显示了我们的 3D 特征提取器。我们首先从 3D 场景中渲染一些**多视图图像**，提取 **2D 密集特征**，然后使用**三种方法**从这些多视图图像**构建 3D 特征**。然后，3D 特征和输入语言提示被输入到 3D-LLM 以生成响应。我们还提出了一种 3D 定位机制，以更好地捕获 3D 空间信息。

><mark style="background: #FFF3A3A6;">这三种方法具体如何构建特征？</mark>

### 4.1 使用2D方式提取3D特征

>利用成熟的2D特征提取器（例如CLIP）

- 从几个不同的视图中**渲染3D场景成2D图像**
- 对2D图像的每个**像素**，利用**2D特征提取器，例如CLIP**提取特征
	- 特征通常为一个高维（如1024维）的向量，包含计算机可以理解的颜色等信息
- 2D特征映射到3D
	- **直接重建法**：使用真实相机参数直接从渲染的RGBD图像中**重建点云**，**特征直接映射到重建的3D点**。这种方法适用于**带相机姿态和内参的渲染RGBD数据**。
	- **特征融合**：作者用 **gradslam** 将**2D特征融合到3D映射中**（？），与密集映射方法不同，这些特征与深度和颜色一起融合。这种方法适用于具有**嘈杂深度图像渲染或嘈杂相机姿态和内参的3D数据**。
	- **神经场**：使用 **神经体素场** 构建3D紧凑表示。具体而言，场中的每个体素都有一个除了密度和颜色的特征。然后使用MSE损失在射线中对齐3D特征，在像素中对齐2D特征。这种方法适用于具有RGB渲染但没有深度数据以及嘈杂相机姿态和内参的3D数据。

### 4.2 训练3D-LLMs

>从头训练3D-LLMs不现实，需要使用2D提取：通常情况下，训练使用了约50亿张图像之后，2D VLMs的训练才开始逐渐有效。它们使用冻结3和预训练的图像编码器（如CLIP）来提取2D图像的特征。由于3D特征提取器可以将3D特征映射到与2D图像相同的特征空间，因此将这些预训练的2D VLMs用作主干简便而合理。


- 预训练大模型：Flamingo和BLIP-2
	- **3D特征提取器与2D特征空间对齐**：由于3D特征与3D特征提取器将其映射到与2D图像相同的特征空间中，因此可以将具有任意大小的点云特征输入到感知器中。因此，作者使用3D特征提取器在与冻结图像编码器特征空间相同的特征空间中提取3D特征。

- 3D **定位机制**
	- 如何将物体的**位置信息**引入到LLM中是另一个问题，一个<mark style="background: #FFF3A3A6;">简单的思路是</mark>，通过已与语言对齐的2D预训练特征提取器（如CLIP和EVA-CLIP）**重建3D特征**后，可以通过直接计算3D特征与语言特征之间的相似性来进行定位。但是作者更希望**模型本身能够捕获3D空间信息**。作者提出了一种3D定位机制，增强了3D-LLMs吸收空间信息的能力。该机制包括两个部分：
		- 用 **position embeddings** 增强3D特征；**position embeddings** 部分是将位置信息添加到从2D多视图特征聚合的3D特征中。
		- 用 **location tokens** 增强 **LLM vocabularies**。位置标记部分则是将3D位置嵌入到词汇表中，使得3D空间位置与LLMs对齐。
			- 







