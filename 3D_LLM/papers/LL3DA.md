LL3DA: Visual Interactive Instruction Tuning for  Omni-3D Understanding, Reasoning, and Planning
## 1. 研究目标

- 开发能够在复杂多样的 3D 环境中理解、推理和规划的 LMM
- 问题：
	- 3D 场景的排列不变点云 3D 表示的需求
- 现有方法：
	- 多视图图像
	- 将 2D 特征投影到 3D 空间作为 3D 场景表示
		- 会导致巨大的计算开销和性能下降

## 2. 主要贡献

- LL3DA：
	- 一种大型语言 3D 助手
	- 采用点云作为直接输入，并对文本指令和视觉提示做出响应
	- 功能：
		- 响应人类的文本和视觉交互
		- 在复杂的 3D 环境中理解、推理和规划
	- 技术：
		- **多模态转换器**：通过注意力机制将来自文本指令、视觉提示和 3D 场景的信息聚合成固定长度的可学习查询标记。
		- **查询标记** 被投影并用作文本指令的前缀，作为预训练和冻结的 LLM 的输入。
		- * 这种设计不仅有助于解决排列不变的 3D 场景嵌入与 LLM 嵌入空间之间的矛盾，而且还可以提取交互感知的 3D 场景嵌入以实现高效的指令跟踪。

## 3. 先前工作

- 3d专家模型
	- 下游任务
		- 3D QA
		- 3D视觉基础、
		- 3D密集字幕
	- 缺陷
		- 监督有限，很难扩大规模以获得更好的性能
- 3DLLM
	- LLM 驱动。聚合了 3D 特征的多视图特征，呈现出机器可以理解各种 3D 对象和场景并遵循人类生成的文本指令的强大能力。
- 提取多视图特征
	- 会导致巨大的计算开销，并且忽略了基本的几何属性。
- 纯文本
	- 常常会导致歧义，尤其是在杂乱且复杂的 3D 环境中。

## 4. 模型介绍

