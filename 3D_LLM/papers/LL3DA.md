LL3DA: Visual Interactive Instruction Tuning for  Omni-3D Understanding, Reasoning, and Planning
## 1. 研究目标

- 开发能够在复杂多样的 3D 环境中理解、推理和规划的 LMM
- 问题：
	- 3D 场景的排列不变点云 3D 表示的需求
- 现有方法：
	- 多视图图像
	- 将 2D 特征投影到 3D 空间作为 3D 场景表示
		- 会导致巨大的计算开销和性能下降

## 2. 主要贡献

- LL3DA：
	- 一种大型语言 3D 助手
	- 采用点云作为直接输入，并对文本指令和视觉提示做出响应
	- 功能：
		- 响应人类的文本和视觉交互
		- 在复杂的 3D 环境中理解、推理和规划
	- 技术：
		- **多模态转换器**：通过注意力机制将来自文本指令、视觉提示和 3D 场景的信息聚合成固定长度的可学习查询标记。
		- **查询标记** 被投影并用作文本指令的前缀，作为预训练和冻结的 LLM 的输入。
		- * 这种设计不仅有助于解决排列不变的 3D 场景嵌入与 LLM 嵌入空间之间的矛盾，而且还可以提取交互感知的 3D 场景嵌入以实现高效的指令跟踪。

## 3. 先前工作

- 3d专家模型
	- 下游任务
		- 3D QA
		- 3D视觉基础、
		- 3D密集字幕
	- 缺陷
		- 监督有限，很难扩大规模以获得更好的性能
- 3DLLM
	- LLM 驱动。聚合了 3D 特征的多视图特征，呈现出机器可以理解各种 3D 对象和场景并遵循人类生成的文本指令的强大能力。
- 提取多视图特征
	- 会导致巨大的计算开销，并且忽略了基本的几何属性。
- 纯文本
	- 常常会导致歧义，尤其是在杂乱且复杂的 3D 环境中。

## 4. 模型介绍

![[Pasted image 20241116200443.png]]
1. **总体流程**包括两个主要步骤：**场景嵌入**与**指令输入**
	1. **提取交互感知的3D场景嵌入**
		- 通过视觉模型（如基于3D点云的模型）
		- **交互感知**指的是场景嵌入信息不仅描述了场景的空间结构，还捕捉了场景中可交互的对象及其关系。
	2. **将场景嵌入信息对齐到文本指令的前缀**
		- 嵌入信息 随后被对齐到与文本指令（如用户输入的自然语言描述）相关的前缀上。这些前缀作为输入传递给一个冻结的大语言模型（LLM）。
		- **注释：
			- 对齐：跨模态的对齐过程，用于将3D场景嵌入的信息转化为与语言模型（LLM）兼容的输入格式。
			- 冻结的LLM表示LLM的参数在整个过程中不更新，只利用其现有的知识进行推理。冻结策略简化了训练，同时保持模型的泛化能力。
2. **Interactor3D模块**：将多模态信息融合（视觉提示、文本指令和3D场景嵌入）到固定长度的查询tokens中
	1. **视觉提示** 是从3D场景嵌入中提取的关键信息，例如场景中的目标对象或区域。（对场景的高层次总结）
	2. **文本指令** 用户通过自然语言提供的任务描述，如“标记桌子上的杯子”