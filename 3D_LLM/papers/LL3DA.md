LL3DA: Visual Interactive Instruction Tuning for  Omni-3D Understanding, Reasoning, and Planning
## 1. 研究目标

- 开发能够在复杂多样的 3D 环境中理解、推理和规划的 LMM
- 问题：
	- 3D 场景的排列不变点云 3D 表示的需求
- 现有方法：
	- 多视图图像
	- 将 2D 特征投影到 3D 空间作为 3D 场景表示
		- 会导致巨大的计算开销和性能下降

## 2. 主要贡献

- LL3DA：
	- 一种大型语言 3D 助手
	- 采用点云作为直接输入，并对文本指令和视觉提示做出响应
	- 功能：
		- 响应人类的文本和视觉交互
		- 在复杂的 3D 环境中理解、推理和规划
	- 技术：
		- **多模态转换器**：通过注意力机制将来自文本指令、视觉提示和 3D 场景的信息聚合成固定长度的可学习查询标记。
		- **查询标记** 被投影并用作文本指令的前缀，作为预训练和冻结的 LLM 的输入。
		- * 这种设计不仅有助于解决排列不变的 3D 场景嵌入与 LLM 嵌入空间之间的矛盾，而且还可以提取交互感知的 3D 场景嵌入以实现高效的指令跟踪。

## 3. 先前工作

- 3d专家模型
	- 下游任务
		- 3D QA
		- 3D视觉基础、
		- 3D密集字幕
	- 缺陷
		- 监督有限，很难扩大规模以获得更好的性能
- 3DLLM
	- LLM 驱动。聚合了 3D 特征的多视图特征，呈现出机器可以理解各种 3D 对象和场景并遵循人类生成的文本指令的强大能力。
- 提取多视图特征
	- 会导致巨大的计算开销，并且忽略了基本的几何属性。
- 纯文本
	- 常常会导致歧义，尤其是在杂乱且复杂的 3D 环境中。

## 4. 问题格式化

- 输入
	- **3D 场景表示**：
	    - 输入形式为一组点云 PC​，其数学表示为 $PC=[p_{in},f_{in}]\in\mathbb{R}^{N\times(3+F)}$
	    - 其中，$p_{in}\in\mathbb{R}^{N\times3}$ 表示点的坐标，而 $f_{in}\in\mathbb{R}^{N\times F}$ 表示附加的点特征，包括颜色、法线方向和高度等。
	- **文本指令**：
	    - 文本指令 $I_t$​ 是模型接收的一部分输入，用于指导模型执行特定任务或操作。这些指令以“### human:”作为标识符开始，表明这是来自用户的输入。
	- **潜在视觉交互**：
		- 视觉交互 $I_v$ 可能作为补充的空间标识符提供给模型，帮助模型更好地理解3D场景中的空间关系和结构。
- 输出
	- **自由形式的自然语言**：
		- 主要为自然语言
		- 部分可以被解析为坐标
		- 采用“### assistant:”标识符来引导模型生成响应
- 坐标表示
	- 将3D坐标信息转换成文本格式，以便现有的大型语言模型（LLMs）能够理解和处理这些信息。
	- 一个**3D点**使用特殊的标签 `<loc>` 包裹起来，格式为 `<loc>x, y, z</loc>`。这里的 `x`, `y`, `z` 分别代表该点在三维空间中的坐标值
	- 一个**3D边界框**则通过它的中心点和大小来表示，格式为 `<obj>c_x, c_y, c_z, w, h, l</obj>`。其中，`c_x`, `c_y`, `c_z` 是边界框中心点的坐标，而 `w`, `h`, `l` 分别表示边界框的宽度、高度和长度。
	- 所有的数值数据都被转换为无符号整数，范围限制在 `[0, 255]` 内。
		- 好处在于，它可以无缝地融入现有预训练语言模型的词汇表中，而无需引入任何额外的学习tokens，从而节省了调整语言模型的工作量。

## 5. 模型简介

![[Pasted image 20241116200443.png]]
### 5.1 总体流程
- **总体流程**包括两个主要步骤：**场景嵌入**与**指令输入**
	1. **提取交互感知的3D场景嵌入**
		- 通过视觉模型（如基于3D点云的模型）
		- **交互感知**指的是场景嵌入信息不仅描述了场景的空间结构，还捕捉了场景中可交互的对象及其关系。
	2. **将场景嵌入信息对齐到文本指令的前缀**
		- 嵌入信息 随后被对齐到与文本指令（如用户输入的自然语言描述）相关的前缀上。这些前缀作为输入传递给一个冻结的大语言模型（LLM）。
		- **注释：
			- 对齐：跨模态的对齐过程，用于将3D场景嵌入的信息转化为与语言模型（LLM）兼容的输入格式。
			- 冻结的LLM表示LLM的参数在整个过程中不更新，只利用其现有的知识进行推理。冻结策略简化了训练，同时保持模型的泛化能力。
### 5.2 Interactor3D模块
- 输入: 将多模态信息融合（视觉提示、文本指令和3D场景嵌入）到固定长度的查询tokens中
	1. **视觉提示** 是从3D场景嵌入中提取的关键信息，例如场景中的目标对象或区域。（对场景的高层次总结）
	2. **文本指令** 用户通过自然语言提供的任务描述，如“标记桌子上的杯子”
	3. **3D场景嵌入**之前有提到
	4. 固定长度的查询tokens
- **三个模块**
	- **冻结的3D场景编码器**
		- 在 ScanNet 目标检测上预训练的 masked transformer encoder
		- 输入: PC点云
		- 输出: 3D场景嵌入
		- $f_{enc}=\mathcal{E}^{3D}\left(PC\right)=\mathcal{E}^{3D}\left(p_{in};f_{in}\right)\in\mathbb{R}^{M\times d}$
	- **视觉prompt编码器**
		- 用户点击
			- 归一化
			- 编码 (3D傅里叶位置编码)
			- $\mathrm{pos}\left(p_{\mathrm{click}}\right)=\left[\sin\left(2\pi p_{\mathrm{click}}\cdot B\right);\cos\left(2\pi p_{\mathrm{click}}\cdot B\right)\right]$
			- 其中 $B\in\mathbb{R}^{3\times(d/2)}$ 是一个可学习矩阵
		- 3D框选
			- 用预训练的3D对象检测器提取ROI特征$f_{\mathrm{box}}\in\mathbb{R}^d$
		- 网络：独立且相同
			- $f_{\mathrm{click}}=FFN_{\mathrm{click}}\left(\mathrm{pos}\left(p_{\mathrm{click}}\right)\right)$
			- $f_{\mathrm{box}}=FFN_{\mathrm{box}}\left(f_{\mathrm{box}}\right)$
	- **多模态Transformer （MMT）**
		- 作用：
			- <mark style="background: #FFB86CA6;">MMT解决了3D场景嵌入（排列不变）与位置敏感的因果语言模型（LLMs）之间的矛盾。</mark>
			- 弥合了冻结的单一模态专家之间的差距。
			- 满足了交互感知特征提取的需求。
		- 工作原理
			- 交叉自注意力机制
			- 从BERT中初始化预训练的词嵌入和位置嵌入。
		- 输出：
			- 32个Query，通过一个简单的线性投影器投影到语言模型的词嵌入空间中。
	- LLM
		- 在推理过程中，生成的响应是通过搜索最优序列 s 来实现的，该序列满足以下条件：
			- $s^*=\arg\max_sP\left(s|PC,\mathcal{I}_t,\mathcal{I}_v\right).$

### 5.3 编码模块
- prompt编码器分别使用位置嵌入和 ROI 特征对用户点击和框坐标进行编码。
	1. 用户框选的目标区域被表示为感兴趣区域（Region of Interest, ROI）的特征向量。


## 6 多模态指令微调

