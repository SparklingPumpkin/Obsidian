
[详解4种模型压缩技术、模型蒸馏算法-CSDN博客](https://blog.csdn.net/woaimx_1314/article/details/131284973)
## 1. 需求

![[Pasted image 20241015142746.png]]


## 2. 方法

![[Pasted image 20241015142809.png]]

- 剪裁：类似“化学结构式的减肥”，将模型结构中对预测结果不重要的网络结构剪裁掉，使网络结构变得更加 ”瘦身“。比如，在每层网络，有些神经元节点的权重非常小，对模型加载信息的影响微乎其微。如果将这些权重较小的神经元删除，则既能保证模型精度不受大影响，又能减小模型大小。
- 量化：类似“量子级别的减肥”，神经网络模型的参数一般都用float32的数据表示，但如果我们将float32的数据计算精度变成int8的计算精度，则可以牺牲一点模型精度来换取更快的计算速度。
- 蒸馏：类似“老师教学生”，使用一个效果好的大模型指导一个小模型训练，因为大模型可以提供更多的软分类信息量，所以会训练出一个效果接近大模型的小模型。
- 神经网络架构搜索（NAS）：类似“化学结构式的重构”，以模型大小和推理速度为约束进行模型结构搜索，从而获得更高效的网络结构。

除此以外，还有权重共享、低秩分解等技术也可实现模型压缩。

## 3. 若干论文

### 3.1 Patient-KD 模型蒸馏

vanilla KD在QNLI和MNLI的训练集上可以很快的达到和teacher model相媲美的性能，但在测试集上则很快达到饱和。对此，作者提出一种假设，在知识蒸馏的过程中过拟合会导致泛化能力不良。为缓解这个问题，论文中提出一种“耐心”师生机制，即让Patient-KD中的学生模型从教师网络的多个中间层进行知识提取，而不是只从教师网络的最后一层输出中学习。

![[Pasted image 20241015153716.png]]

1. PKD-Skip: 从每k层学习，这种策略是假设网络的底层包含重要信息，需要被学习到（如图2a所示）
2. PKD-last: 从最后k层学习，假设教师网络越靠后的层包含越丰富的知识信息（如图2b所示）
3. ![[Pasted image 20241015153946.png]]
