参考资料:
- [大模型知识蒸馏概述 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/659943824)
## 1. 大模型压缩

- [剪枝](https://zhida.zhihu.com/search?content_id=234782574&content_type=Article&match_order=1&q=%E5%89%AA%E6%9E%9D&zhida_source=entity)（Pruning）
- [知识蒸馏](https://zhida.zhihu.com/search?content_id=234782574&content_type=Article&match_order=1&q=%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F&zhida_source=entity)（Knowledge Distillation）
- 量化（Quantization）
- 低秩分解（Low-Rank Factorization）

## 2. 知识蒸馏简介

将大型模型（教师模型）的[知识迁移](https://zhida.zhihu.com/search?content_id=234782574&content_type=Article&match_order=1&q=%E7%9F%A5%E8%AF%86%E8%BF%81%E7%A7%BB&zhida_source=entity)到较小的模型（学生模型）中。 KD背后的核心思想是将教师模型的综合知识转化为更精简、更有效的表示。

接下来本博客将概述利用LLM作为教师的蒸馏方法。根据这些方法是否将LLM的涌现能力（EA）提炼成小语言模型（SLM）来对这些方法进行分类。

因此，我们将这些方法分为两个不同的类别：
- 标准 KD 
- 基于 EA 的 KD
 ![[v2-ce310174973a29c552c7d3de7c606815_1440w.webp]]