
- [参考](https://zhuanlan.zhihu.com/p/659943824)
- [参考](https://blog.csdn.net/qq_36426650/article/details/128083896)
## 1. 大模型压缩

- [剪枝](https://zhida.zhihu.com/search?content_id=234782574&content_type=Article&match_order=1&q=%E5%89%AA%E6%9E%9D&zhida_source=entity)（Pruning）
- [知识蒸馏](https://zhida.zhihu.com/search?content_id=234782574&content_type=Article&match_order=1&q=%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F&zhida_source=entity)（Knowledge Distillation）
- 量化（Quantization）
- 低秩分解（Low-Rank Factorization）

## 2. 知识蒸馏简介

将大型模型（教师模型）的[知识迁移](https://zhida.zhihu.com/search?content_id=234782574&content_type=Article&match_order=1&q=%E7%9F%A5%E8%AF%86%E8%BF%81%E7%A7%BB&zhida_source=entity)到较小的模型（学生模型）中。 KD背后的核心思想是将教师模型的综合知识转化为更精简、更有效的表示。

接下来本博客将概述利用LLM作为教师的蒸馏方法。根据这些方法是否将LLM的涌现能力（EA）提炼成小语言模型（SLM）来对这些方法进行分类。

因此，我们将这些方法分为两个不同的类别：
- 标准 KD 
- 基于 EA 的 KD
 ![[v2-ce310174973a29c552c7d3de7c606815_1440w.webp]]
## 3. 标准知识蒸馏

Standard KD旨在使学生模型学习LLM所拥有的常见知识，如**输出分布和特征信息**。这种方法类似于传统的KD，但区别在于教师模型是LLM。 比如：MINILLM 和 GKD。

- MINILLM （**Knowledge Distillation of Large Language Models**）
	- 深入研究了白盒生成LLM的蒸馏。 它观察到最小化前向 Kullback-Leibler 散度 (KLD) 的挑战（这可能会导致教师分布中不太可能的区域出现概率过高，从而在自由运行生成过程中导致不可能的样本）。为了解决这个问题，MINILLM 选择最小化逆向 KLD。 这种方法可以**防止学生高估教师分布中的[低概率区域](https://zhida.zhihu.com/search?content_id=234782574&content_type=Article&match_order=1&q=%E4%BD%8E%E6%A6%82%E7%8E%87%E5%8C%BA%E5%9F%9F&zhida_source=entity)，从而提高生成样本的质量**。
- GKD（**GKD: Generalized Knowledge Distillation for Auto-regressive Sequence Models**） 
	- 探索了[自回归模型](https://zhida.zhihu.com/search?content_id=234782574&content_type=Article&match_order=1&q=%E8%87%AA%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B&zhida_source=entity)的蒸馏，这里白盒生成 LLM 是一个子集。 该方法确定了两个关键问题：
	- 1. 训练期间的输出序列与学生在部署期间生成的输出序列之间的分布不匹配
	- 2. 模型under-specification，其中学生模型可能缺乏与教师分布相匹配的表达能力。
	- GKD 通过在训练期间对学生的输出序列进行采样来处理分布不匹配，它还通过优化替代散度（逆向 KL）来解决模型under-specification的问题。

## 4. **基于涌现能力的知识蒸馏**

基于 EA 的 KD 不仅仅迁移 LLM 的常识，还包括蒸馏他们的**涌现能力**。
![[v2-acd06058251149eb0671b00d437fbfde_1440w.webp]]
### 4.1 **上下文学习蒸馏**

ICL 采用结构化自然语言提示，其中包含任务描述以及可能的一些任务示例作为演示。**通过这些任务示例，LLM可以掌握并执行新任务，而无需显式梯度更新**。

- **In-context Learning Distillation: Transferring Few-shot Learning Ability of Pre-trained Language Models** 
	- 论文中引入了 ICL 蒸馏，它将上下文小样本学习和语言建模功能从 LLM 转移到 SLM。这是通过将上下文学习目标与传统语言建模目标相结合来实现的。
	- 为了实现这一目标，他们在两种小样本学习范式下探索了 ICL 蒸馏：元上下文调优 (Meta-ICT) 和多任务上下文调优 (Multitask-ICT)。
		- ![[v2-013cedf9c7c835a19cecc3ef6bb14c96_1440w.webp]]
	- **元上下文调优 (Meta-ICT)**
		- 即, 把元学习和In-context Tuning结合起来, 让SLM拥有 "学习怎么去ICL" 的能力, 也就是把EA能力蒸馏到了小模型上. 
		- 在 Meta-ICT 中，语言模型使用ICL objectives 在不同任务中进行元训练。这使其能够通过上下文学习来适应看不见的任务，从而扩展其解决问题的能力。
	- **多任务上下文调优 (Multitask-ICT)**
		- Multitask-ICT 使用 (ICL objectives) 和 (target tasks 中的一些示例) 对模型进行微调。随后，它采用上下文学习来对这些任务进行预测。
- **Meta-learning via Language Model In-context Tuning**
	- 在训练（fine-tuning）阶段，给定一系列的训练task，每一个task都有相应的instruction，以及该task对应的少量样本（输入/输出对）。在测试阶段，给定一个新的unseen task，以及该task对应的instruction和少量样本（输入/输出对），旨在让模型能够对测试样本预测其类别。


### 4.2 **思维链蒸馏**

与 ICL 相比，CoT 采用了不同的方法，它将中间推理步骤（可以导出最终输出）合并到提示中，而不是使用简单的输入输出对。

- MT-CoT （**Explanations from Large Language Models Make Small Reasoners Better**） 
	- 旨在利用 LLM 产生的CoT来加强小型推理机的训练。它利用多任务学习框架使较小的模型具有强大的推理能力以及生成解释的能力。
	- ![[v2-d4441f0eb1dfba4ad38f5cd52c2847a1_1440w.webp]]
- Fine-tune CoT （**Large language models are reasoning teachers**）
	- 更进一步，通过随机采样从 LLM 生成多个推理解决方案。 训练数据的增强有助于学生模型的学习过程。
	- ![[v2-0fa574e769dc8c5963a205cfe9b66af5_1440w.webp]]
	- 

### 4.3 **指令遵循蒸馏**

